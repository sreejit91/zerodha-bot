{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "31d84c093123f7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Enhanced Smoke-Test: Simulate OHLC ticks ────────────────────────────\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import timezone, timedelta\n",
    "import os, shutil\n",
    "\n",
    "from config import load_config\n",
    "from broker import KiteWrapper, _resolve_token\n",
    "from tick_aggregator import TickAggregator\n",
    "\n",
    "# 1) Clean test directory\n",
    "TEST_DIR = \"data_test\"\n",
    "if os.path.exists(TEST_DIR):\n",
    "    shutil.rmtree(TEST_DIR)\n",
    "os.makedirs(TEST_DIR)\n",
    "\n",
    "# 2) Load 1 day of 3-min bars (our ground truth)\n",
    "cfg = load_config()\n",
    "kw  = KiteWrapper(cfg)\n",
    "df3 = kw.history(days=1, interval=\"3minute\", tradingsymbol=\"RELIANCE\")\n",
    "\n",
    "# 3) Instatiate aggregator for 180s bars\n",
    "agg = TickAggregator(kw, symbol=\"RELIANCE\", intervals=[180], data_dir=TEST_DIR)\n",
    "\n",
    "# 4) Build synthetic ticks: OHLC for each bar\n",
    "ticks = []\n",
    "for ts, row in df3.iterrows():\n",
    "    # convert IST ts -> UTC iso for Kite-format\n",
    "    utc_iso = ts.tz_localize(\"Asia/Kolkata\") \\\n",
    "                 .astimezone(timezone.utc) \\\n",
    "                 .isoformat()\n",
    "    # feed four ticks per bar: open, high, low, close\n",
    "    for price in (row[\"open\"], row[\"high\"], row[\"low\"], row[\"close\"]):\n",
    "        ticks.append({\n",
    "            \"instrument_token\": agg.token,\n",
    "            \"last_price\":       price,\n",
    "            \"last_trade_time\":  utc_iso\n",
    "        })\n",
    "\n",
    "# 5) Add flush tick at last_bar_start + interval\n",
    "last_start = df3.index[-1]\n",
    "flush_time = last_start + timedelta(seconds=180)\n",
    "utc_flush  = flush_time.tz_localize(\"Asia/Kolkata\") \\\n",
    "                      .astimezone(timezone.utc) \\\n",
    "                      .isoformat()\n",
    "ticks.append({\n",
    "    \"instrument_token\": agg.token,\n",
    "    \"last_price\":       df3.iloc[-1][\"close\"],\n",
    "    \"last_trade_time\":  utc_flush\n",
    "})\n",
    "\n",
    "# 6) Feed ticks into the aggregator\n",
    "for tick in ticks:\n",
    "    agg.on_tick(tick)\n",
    "\n",
    "# 7) Read back generated CSV\n",
    "out_path = os.path.join(TEST_DIR, \"RELIANCE_180sec.csv\")\n",
    "df_out   = pd.read_csv(out_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# 8) Build expected DF (only OHLC)\n",
    "df_exp = df3[[\"open\",\"high\",\"low\",\"close\"]].copy()\n",
    "df_exp.index.name = df_out.index.name\n",
    "\n",
    "# 9) Compare OHLC & structure\n",
    "rows_ok = len(df_exp) == len(df_out)\n",
    "dups_ok = df_out.index.duplicated().sum() == 0\n",
    "ohlc_ok = df_exp.equals(df_out[[\"open\",\"high\",\"low\",\"close\"]])\n",
    "\n",
    "print(f\"Rows expected: {len(df_exp)}, output: {len(df_out)}\")\n",
    "print(\"No duplicate timestamps:\", dups_ok)\n",
    "print(\"OHLC exact match:\", ohlc_ok)\n",
    "\n",
    "if rows_ok and dups_ok and ohlc_ok:\n",
    "    print(\"\\n✅ TickAggregator smoke-test PASSED!\")\n",
    "else:\n",
    "    print(\"\\n❌ TickAggregator smoke-test FAILED — inspect diffs below.\")\n",
    "    display(df_exp.head(3).rename(columns=lambda c: f\"exp_{c}\"))\n",
    "    display(df_out.head(3))\n"
   ],
   "id": "9a2640490d256843"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ─── Fixed Integrated Smoke-Test for ALL FEATURES ────────────────────────\n",
    "\n",
    "# 0) Monkey-patch numpy so pandas_ta can import\n",
    "import numpy as np\n",
    "if not hasattr(np, \"NaN\"):\n",
    "    np.NaN = np.nan\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from features import add_indicators, ENABLED\n",
    "\n",
    "# 1) Load sample bars\n",
    "df = pd.read_csv(\"data/RELIANCE_5minute.csv\",\n",
    "                 index_col=0, parse_dates=True).ffill()\n",
    "\n",
    "# 2) Compute all enabled features\n",
    "df_feat = add_indicators(df, debug=False)\n",
    "\n",
    "# 3) Collect mismatches\n",
    "errors = []\n",
    "\n",
    "# ret1\n",
    "if \"ret1\" in ENABLED:\n",
    "    exp = df[\"close\"].pct_change().fillna(0)\n",
    "    if not df_feat[\"ret1\"].equals(exp):\n",
    "        errors.append(\"ret1\")\n",
    "\n",
    "# EMA_n\n",
    "for feat in ENABLED:\n",
    "    if feat.startswith(\"ema_\"):\n",
    "        n   = int(feat.split(\"_\")[1])\n",
    "        exp = df[\"close\"].ewm(span=n, adjust=False).mean()\n",
    "        if not df_feat[f\"ema_{n}\"].equals(exp):\n",
    "            errors.append(f\"ema_{n}\")\n",
    "\n",
    "# RSI_n\n",
    "for feat in ENABLED:\n",
    "    if feat.startswith(\"rsi_\"):\n",
    "        n   = int(feat.split(\"_\")[1])\n",
    "        exp = ta.rsi(df[\"close\"], length=n)\n",
    "        exp = pd.Series(exp, index=df.index, name=f\"rsi_{n}\")\n",
    "        if not df_feat[f\"rsi_{n}\"].equals(exp):\n",
    "            errors.append(f\"rsi_{n}\")\n",
    "\n",
    "# MACD\n",
    "if \"macd\" in ENABLED:\n",
    "    macd = ta.macd(df[\"close\"], fast=12, slow=26, signal=9)\n",
    "    for col in macd.columns:\n",
    "        if not df_feat[col].equals(macd[col]):\n",
    "            errors.append(col)\n",
    "\n",
    "# VWAP\n",
    "if \"vwap\" in ENABLED:\n",
    "    exp = ta.vwap(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"])\n",
    "    if not df_feat[\"vwap\"].equals(exp):\n",
    "        errors.append(\"vwap\")\n",
    "\n",
    "# vol_spike\n",
    "if \"vol_spike\" in ENABLED:\n",
    "    vol_avg = df[\"volume\"].rolling(20, min_periods=1).mean()\n",
    "    exp     = (df[\"volume\"] > 2 * vol_avg).astype(int)\n",
    "    if not df_feat[\"vol_spike\"].equals(exp):\n",
    "        errors.append(\"vol_spike\")\n",
    "\n",
    "# Bollinger\n",
    "if \"bollinger\" in ENABLED:\n",
    "    bb = ta.bbands(df[\"close\"], length=20, std=2)\n",
    "    mapping = {\n",
    "      \"bb_mid\":   \"BBM_20_2.0\",\n",
    "      \"bb_lower\": \"BBL_20_2.0\",\n",
    "      \"bb_upper\": \"BBU_20_2.0\",\n",
    "    }\n",
    "    for tgt, src in mapping.items():\n",
    "        if not df_feat[tgt].equals(bb[src]):\n",
    "            errors.append(tgt)\n",
    "\n",
    "# Stochastic\n",
    "if \"stochastic\" in ENABLED:\n",
    "    low  = df[\"low\"].rolling(14, min_periods=1).min()\n",
    "    high = df[\"high\"].rolling(14, min_periods=1).max()\n",
    "    exp_k = 100 * (df[\"close\"] - low) / (high - low)\n",
    "    exp_d = exp_k.rolling(3, min_periods=1).mean()\n",
    "    if not df_feat[\"stoch_k\"].equals(exp_k):\n",
    "        errors.append(\"stoch_k\")\n",
    "    if not df_feat[\"stoch_d\"].equals(exp_d):\n",
    "        errors.append(\"stoch_d\")\n",
    "\n",
    "# OBV\n",
    "if \"obv\" in ENABLED:\n",
    "    exp = ta.obv(df[\"close\"], df[\"volume\"])\n",
    "    if not df_feat[\"obv\"].equals(exp):\n",
    "        errors.append(\"obv\")\n",
    "\n",
    "# SuperTrend\n",
    "if \"supertrend\" in ENABLED:\n",
    "    st = ta.supertrend(df[\"high\"], df[\"low\"], df[\"close\"], length=10, multiplier=3)\n",
    "    if not df_feat[\"supertrend\"].equals(st[\"SUPERT_10_3.0\"]):\n",
    "        errors.append(\"supertrend\")\n",
    "    if not df_feat[\"supertrend_dir\"].equals(st[\"SUPERTd_10_3.0\"]):\n",
    "        errors.append(\"supertrend_dir\")\n",
    "\n",
    "# 4) Report\n",
    "if errors:\n",
    "    print(\"❌ Mismatches in:\", errors)\n",
    "else:\n",
    "    print(\"✅ All enabled indicators match perfectly.\")\n"
   ],
   "id": "2760f560094f7f8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_feat",
   "id": "c584d4dd9258a16e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load the JSONL tick file into a DataFrame\n",
    "file_path = \"live_tick_data/RELIANCE_ticks.jsonl\"  # adjust if you used a different folder or symbol\n",
    "df_ticks = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# 2) Inspect the columns and their data types\n",
    "print(\"Columns and dtypes:\")\n",
    "print(df_ticks.dtypes)\n",
    "\n",
    "# 3) Show the first 5 rows\n",
    "print(\"\\nFirst 5 ticks:\")\n",
    "print(df_ticks.head())\n",
    "\n",
    "# 4) Summarize timestamp range and row count\n",
    "print(\"\\nTimestamp range and total rows:\")\n",
    "print(\"  min:\", df_ticks[\"last_trade_time\"].min())\n",
    "print(\"  max:\", df_ticks[\"last_trade_time\"].max())\n",
    "print(\"  total rows:\", len(df_ticks))\n"
   ],
   "id": "f5489600151b8a9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f3c1df8b0c526afc"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
